{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example usage:\n",
      "[['i', 'like', 'to', 'eat', 'pie']]\n",
      "[['eat', 'pie', 'be', 'extremely', 'difficult']]\n",
      "[['be', 'you', 'my', 'friend', '?', '-fine', ',', 'thanks', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from typing import List\n",
    "\n",
    "# lemmatization with wordnet and lexicon creation\n",
    "POS_TAGS_TO_IGNORE = ['CC', 'DT', 'EX', 'LS', 'PDT', 'POS', 'RP', 'UH', 'WDT', 'WP', 'WP$', 'WRB', 'MD']\n",
    "POS_TAGS_TO_LEAVE_AS_IS = ['CD', 'FW', 'IN', 'PRP', 'PRP$']\n",
    "POS_TAGS_TO_LEMMATIZE = ['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP',\n",
    "                         'VBZ']\n",
    "\n",
    "\n",
    "def get_wordnet_pos(pos_tag: str):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return nltk.corpus.reader.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return nltk.corpus.reader.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return nltk.corpus.reader.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return nltk.corpus.reader.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def lemmatize_with_wordnet(word: str, pos_tag: str, lemmatizer: WordNetLemmatizer):\n",
    "    wordnet_pos = get_wordnet_pos(pos_tag)\n",
    "    if wordnet_pos is not None:\n",
    "        return lemmatizer.lemmatize(word, wordnet_pos)\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def prune_tokens_based_on_pos(word: str, pos_tag: str, lemmatizer: WordNetLemmatizer):\n",
    "    if pos_tag in POS_TAGS_TO_IGNORE:\n",
    "        return None\n",
    "    elif pos_tag in POS_TAGS_TO_LEAVE_AS_IS:\n",
    "        return word\n",
    "    elif pos_tag in POS_TAGS_TO_LEMMATIZE:\n",
    "        return lemmatize_with_wordnet(word, pos_tag, lemmatizer)\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "def prune_token_list(tokens: List[str], lemmatizer: WordNetLemmatizer) -> List[List[str]]:\n",
    "    tokens_with_pos = nltk.pos_tag(tokens)\n",
    "    pruned = [prune_tokens_based_on_pos(token[0], token[1], lemmatizer) for token in tokens_with_pos]\n",
    "    return list(filter(lambda x: x is not None, pruned))\n",
    "\n",
    "\n",
    "def tokenize_and_lemmatize_tweets(tweets: List[str]) -> List[List[str]]:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    pruned_tweets = []\n",
    "    count = 0\n",
    "    for tweet in tweets:\n",
    "        count += 1\n",
    "        tokenized_tweet = nltk.word_tokenize(tweet.lower())\n",
    "        pruned_tweet = prune_token_list(tokenized_tweet, lemmatizer)\n",
    "        pruned_tweets.append(pruned_tweet)\n",
    "        if count % 1000 == 0:\n",
    "            print(\"Processed tweets: \",count)\n",
    "            \n",
    "    return pruned_tweets\n",
    "\n",
    "# end of lexicon creation and lemmatization\n",
    "\n",
    "print(\"Example usage:\")\n",
    "print(tokenize_and_lemmatize_tweets([\"i like to eat pie\"]))\n",
    "print(tokenize_and_lemmatize_tweets([\"eating pie can be extremely difficult\"]))\n",
    "print(tokenize_and_lemmatize_tweets([\"How are you my friend? -Fine, thanks.\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_pattern_from_string(given_string, re_compiled_pattern):\n",
    "    \"\"\"removes a re compiled regex pattern from a given string \"\"\"\n",
    "    return re_compiled_pattern.sub(\"\", given_string)\n",
    "\n",
    "def remove_links_from_tweets(tweets_list: List[str]) -> List[str]: \n",
    "    \"\"\"for a given list of tweet texts removes all links starting with http:// or https://\"\"\"\n",
    "    regex = re.compile(\"(https?://\\S+)\", re.IGNORECASE)\n",
    "    tweets_without_links = []\n",
    "    for tweet in tweets_list:\n",
    "        clean_tweet = remove_pattern_from_string(tweet, regex)\n",
    "        tweets_without_links.append(clean_tweet)\n",
    "    return tweets_without_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'like', 'pie', '#fire', 'bableh', '3rd']  ->  ['i', 'like', 'pie', 'fire']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "letters_only_regex = re.compile(\"[^a-zA-Z]\")\n",
    "\n",
    "def keep_only_wordnet_tokens(tweet_tokens:List[str]) -> List[str]:\n",
    "    tweet_tokens = [letters_only_regex.sub(\"\", word) for word in tweet_tokens]\n",
    "    return [word for word in tweet_tokens if wordnet.synsets(word)]\n",
    "    \n",
    "example_tokens = ['i', 'like', 'pie', '#fire', 'bableh','3rd']\n",
    "pruned_tokens = keep_only_wordnet_tokens(example_tokens)\n",
    "print(example_tokens, ' -> ', pruned_tokens)\n",
    "\n",
    "def keep_only_wordnet_tokens_in_list_of_tweets(tweets: List[List[str]]) -> List[List[str]]:\n",
    "    return [keep_only_wordnet_tokens(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tweets_tralala(tweet_list:List[str]) -> List[str]:\n",
    "    modified_tweets = remove_links_from_tweets(tweet_list)\n",
    "    modified_tweets = tokenize_and_lemmatize_tweets(modified_tweets)\n",
    "    modified_tweets = keep_only_wordnet_tokens_in_list_of_tweets(modified_tweets)\n",
    "    return modified_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tuple_from_input_file(lines_with_tweet_and_class, delimiter):\n",
    "    \"\"\"splits each tuple (tweet, class) and appends them to tweets[] and classes[] accordingly and \n",
    "        returns them as (tweets, classes)\"\"\"\n",
    "    tweets = []\n",
    "    classes = []\n",
    "    for tweet in lines_with_tweet_and_class:\n",
    "        splits = tweet.split(delimiter)\n",
    "        tweets.append(splits[0])\n",
    "        classes.append(splits[1])\n",
    "\n",
    "    return tweets, classes\n",
    "\n",
    "\n",
    "def get_tuple_from_test_input_file(tweets_with_number, delimiter):\n",
    "    \"\"\"splits each tuple (index, tweet) adding the results in into tweets and indexes returns (tweets, indexes)\"\"\"\n",
    "    tweets = []\n",
    "    index_numbers = []\n",
    "    for tweet in tweets_with_number:\n",
    "        splits = tweet.split(delimiter)\n",
    "        tweets.append(splits[1])\n",
    "        index_numbers.append(splits[0])\n",
    "\n",
    "    return tweets, index_numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "n_classes = 3\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "def convert_question_to_vector(word_centroid_map, tweet: List[str]) -> np.ndarray:\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count\n",
    "    # by one\n",
    "    for word in tweet:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_tweets_to_feature_vectors(centroid_map, clean_tweets: List[List[str]]) -> List[np.ndarray]:\n",
    "    return [convert_question_to_vector(centroid_map, tweet) for tweet in clean_tweets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def neural_network_model(data, num_input:int):\n",
    "    print(\"creating network model\")\n",
    "    hidden_1_layer = {'weights': tf.Variable(tf.random_normal([num_input, n_nodes_hl1])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                    'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3, output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input from:  train_and_dev_data/tweet_input/train_input.tsv\n",
      "Processed tweets:  1000\n",
      "Processed tweets:  2000\n",
      "Processed tweets:  3000\n",
      "Processed tweets:  4000\n",
      "Example converted classes:  [[0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def convert_class(class_of_tweet:str) -> List[int]:\n",
    "    if class_of_tweet.strip() == 'positive':\n",
    "        return [1,0,0]\n",
    "    elif class_of_tweet.strip() == 'negative':\n",
    "        return [0,0,1]\n",
    "    return [0,1,0]\n",
    "\n",
    "def convert_classes(classes: List[str]) -> List[List[int]]:\n",
    "    return [convert_class(class_of_tweet) for class_of_tweet in classes]\n",
    "\n",
    "input_lines = []\n",
    "\n",
    "# training data set, each line = (tweet, class)\n",
    "train_file_name = \"train_and_dev_data/tweet_input/train_input.tsv\"\n",
    "\n",
    "# test data set, each line = (index/ line no., tweet)\n",
    "test_file_name = \"train_and_dev_data/tweet_input/test_input.tsv\"\n",
    "\n",
    "# solutions file, each line = (index, correct class)\n",
    "solutions_file_name = \"train_and_dev_data/tweet_output/test_solutions.tsv\"\n",
    "\n",
    "print(\"Reading input from: \", train_file_name)\n",
    "with open(train_file_name) as f:\n",
    "    input_lines = f.readlines()\n",
    "\n",
    "# convert from (tweet, class)[] to (tweet[], class[])\n",
    "tweets_and_class_tuple = get_tuple_from_input_file(input_lines, \"\\t\")\n",
    "clean_tweets = clean_tweets_tralala(tweets_and_class_tuple[0])\n",
    "\n",
    "converted_classes = convert_classes(tweets_and_class_tuple[1])\n",
    "print(\"Example converted classes: \", converted_classes[0:10])\n",
    "\n",
    "idx = pickle.load(open(\"train_and_dev_data/neural_model/idx\", \"rb\"))\n",
    "index2word = pickle.load(open(\"train_and_dev_data/neural_model/index2word\", \"rb\"))\n",
    "word_centroid_map = dict(zip(index2word, idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(clean_tweets[0:3])\n",
    "train_tweet_features = convert_tweets_to_feature_vectors(word_centroid_map,clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test input from:  train_and_dev_data/tweet_input/test_input.tsv\n",
      "Processed tweets:  1000\n",
      "Processed tweets:  2000\n",
      "Processed tweets:  3000\n",
      "Processed tweets:  4000\n",
      "Processed tweets:  5000\n",
      "Processed tweets:  6000\n",
      "Processed tweets:  7000\n",
      "Processed tweets:  8000\n",
      "Processed tweets:  9000\n",
      "Processed tweets:  10000\n",
      "Processed tweets:  11000\n",
      "Processed tweets:  12000\n",
      "Processed tweets:  13000\n",
      "Processed tweets:  14000\n",
      "Processed tweets:  15000\n",
      "Processed tweets:  16000\n",
      "Processed tweets:  17000\n",
      "Processed tweets:  18000\n",
      "Processed tweets:  19000\n",
      "Processed tweets:  20000\n",
      "Processed tweets:  21000\n",
      "Processed tweets:  22000\n",
      "Processed tweets:  23000\n",
      "Processed tweets:  24000\n",
      "Processed tweets:  25000\n",
      "Processed tweets:  26000\n",
      "Processed tweets:  27000\n",
      "Processed tweets:  28000\n",
      "Processed tweets:  29000\n",
      "Processed tweets:  30000\n",
      "Processed tweets:  31000\n",
      "Processed tweets:  32000\n",
      "[1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Reading test input from: \", test_file_name)\n",
    "with open(test_file_name) as f:\n",
    "    test_input_lines = f.readlines()\n",
    "\n",
    "# convert from (tweet, class)[] to (tweet[], class[])\n",
    "test_tweets_and_class_tuple = get_tuple_from_test_input_file(test_input_lines, \"\\t\")\n",
    "clean_test_tweets = clean_tweets_tralala(test_tweets_and_class_tuple[0])\n",
    "\n",
    "test_tweet_features = convert_tweets_to_feature_vectors(word_centroid_map, clean_test_tweets)\n",
    "\n",
    "solutions = []\n",
    "with open(solutions_file_name) as f:\n",
    "    solutions = f.readlines()\n",
    "\n",
    "solution_and_index = get_tuple_from_test_input_file(solutions, \"\\t\")\n",
    "solutions = solution_and_index[0]\n",
    "\n",
    "converted_solutions = convert_classes(solutions)\n",
    "print(converted_solutions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_neural_network(x, train_data: Tuple[List, List], test_data: Tuple[List, List]):\n",
    "    print(\"training neural network\")\n",
    "    num_input = len(train_data[0][0])\n",
    "    prediction = neural_network_model(x, num_input)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    hm_epochs = 15\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(hm_epochs):\n",
    "            print(\"starting epoch\", epoch)\n",
    "            epoch_loss = 0\n",
    "            for num_batch in range(int(len(train_data[0]) / batch_size)):\n",
    "                epoch_x = train_data[0][num_batch * batch_size: (num_batch + 1) * batch_size]\n",
    "                epoch_y = train_data[1][num_batch * batch_size:(num_batch + 1) * batch_size]\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "\n",
    "            print('Epoch', epoch, ' completed out of ', hm_epochs, '; loss:', epoch_loss)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:', accuracy.eval({x: test_data[0], y: test_data[1]}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4954\n",
      "4954\n",
      "32009\n",
      "32009\n",
      "training neural network\n",
      "creating network model\n",
      "starting epoch 0\n",
      "Epoch 0  completed out of  15 ; loss: 292029.871826\n",
      "starting epoch 1\n",
      "Epoch 1  completed out of  15 ; loss: 155790.733398\n",
      "starting epoch 2\n",
      "Epoch 2  completed out of  15 ; loss: 100003.949341\n",
      "starting epoch 3\n",
      "Epoch 3  completed out of  15 ; loss: 67766.3956909\n",
      "starting epoch 4\n",
      "Epoch 4  completed out of  15 ; loss: 48861.2230835\n",
      "starting epoch 5\n",
      "Epoch 5  completed out of  15 ; loss: 39694.5162048\n",
      "starting epoch 6\n",
      "Epoch 6  completed out of  15 ; loss: 40595.4373169\n",
      "starting epoch 7\n",
      "Epoch 7  completed out of  15 ; loss: 35746.3953094\n",
      "starting epoch 8\n",
      "Epoch 8  completed out of  15 ; loss: 25100.2447281\n",
      "starting epoch 9\n",
      "Epoch 9  completed out of  15 ; loss: 11544.8851852\n",
      "starting epoch 10\n",
      "Epoch 10  completed out of  15 ; loss: 7746.88219619\n",
      "starting epoch 11\n",
      "Epoch 11  completed out of  15 ; loss: 7240.78979685\n",
      "starting epoch 12\n",
      "Epoch 12  completed out of  15 ; loss: 8274.92625904\n",
      "starting epoch 13\n",
      "Epoch 13  completed out of  15 ; loss: 3647.25352955\n",
      "starting epoch 14\n",
      "Epoch 14  completed out of  15 ; loss: 1181.469859\n",
      "Accuracy: 0.435784\n"
     ]
    }
   ],
   "source": [
    "print(len(train_tweet_features))\n",
    "print(len(converted_classes))\n",
    "\n",
    "print(len(test_tweet_features))\n",
    "print(len(converted_solutions))\n",
    "\n",
    "_x = tf.placeholder('float', [None, len(train_tweet_features[0])])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "train_neural_network(_x, (train_tweet_features, converted_classes), (test_tweet_features, converted_solutions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input from:  train_and_dev_data/tweet_input/train_input.tsv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
